<html>
  <head>
    <meta charset="UTF-8">
    <style type="text/css">
      #title {text-align: center; font-family: arial;}
      #authors {text-align: center; font-style: italic; font-size: 19px; font-family: arial;}
      #abstract {text-align: center; font-family: arial;}
      #Audio-Samples {text-align: center; font-family: arial;}
      #Audio-Samples-detail {text-align: center; font-weight: normal; font-size: 17px; font-family: arial;}

      .container {width: 900px; margin: 30px auto;}
      .container2 {width: 900px; margin: 30px auto;}
      .centered {position: relative;display: inline-block; padding: 1em;font-weight: normal;font-size: 17px;font-family: arial;}
      .sample {text-align: center; font-family: arial;}
      .container-sample1 {width: 1500px; margin: 5px auto;}
      .centered-sample1 {position: relative;display: inline-block;padding: 1em;font-weight: normal;font-size: 17px;font-family: arial;}
      .sample1 {font-style: italic; font-family: arial;}
      .grey {font-style: italic; font-size: 15px; font-family: arial;}
    </style>
  </head>

  <body>
    <h1 id="title"><br />KES-TTS: A Keyword-Aware, Emotion-Driven, and Speaker-Controllable TTS</h1>
    <p id="authors">
      Kyungseok Oh<sup>a</sup>, Rakbeen Song<sup>a</sup>, Chulwon Choi<sup>a</sup>, 
      Bonhwa Ku<sup>a</sup>, Hanseok Ko<sup>b</sup><sup>*</sup>
    </p>
    
    <p id="affiliations" style="text-align: center; font-size: 0.9em; font-style: italic;">
      <sup>a</sup>Korea University, Department of Electrical and Computer Engineering<br>
      <sup>b</sup>Catholic University of America, Department of Electrical Engineering and Computer Science
    </p>
    

    <h2 id="abstract"><br /><br />Abstract</h2>
    <div class="container">
      <div class="centered">
Text-to-Speech (TTS) plays a crucial role in advancing human-computer communication, enabling more natural and effective interactions across various applications. A key requirement for achieving human-like TTS is the ability to generate emotionally expressive and speaker-controllable speech while appropriately emphasizing keywords within a sentence. However, conventional TTS models often fail to capture word-level prosodic variations and lack mechanisms for automatically detecting linguistically and acoustically significant words, resulting in monotonous and less expressive speech. To address these challenges, we propose KES-TTS, which achieves keyword-aware, emotion-driven, and speaker-controllable speech synthesis by integrating automatic keyword detection using KeyBERT, CLN-based style injection, and word-level emphasis control. This enables more natural, expressive, and contextually adaptive prosody. KES-TTS is evaluated on the Emotional Speech Dataset (ESD) and demonstrates significant improvements in speech naturalness, emotional expressiveness, and speaker similarity through both objective and subjective evaluations. Ablation studies and keyword alignment analyses further validate the effectiveness of the proposed emphasis control and keyword detection strategies, confirming that KES-TTS effectively addresses the challenges of multi-speaker, multi-emotion expressive TTS.
      </div>
    </div>

  <div style="text-align: center; max-width: 800px; margin: 0 auto;">
    <img src="Architecture.PNG" alt="Model Figure" style="width: 100%; max-width: 700px;"><br>
    <p style="font-size: 0.85em; font-style: italic; line-height: 1.5; margin-top: 8px;">
      (a) illustrates the structure of KES-TTS during the training step, while (b) presents the architecture of the emphasis predictors.<br>
      The emphasis predictors consist of three predictors with the same structure, each designed to estimate variations in pitch and duration.<br>
      (c) depicts the structure of the discriminator, and (d) shows the structure of KES-TTS during the inference step.<br>
      Keyword detection automatically identifies linguistically and acoustically important words from the text input.
    </p>
  </div>
    
  </body>
</html>
